{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Estos son los imports que se necesitan para poder activar el codido o poder usarlo\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "sys.modules['keras'] = tf.keras  # Para que vit_keras use tf.keras internamente\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Ruta a tu proyecto (PainClassifier)\n",
    "sys.path.append(\n",
    "    \"C:\\\\Users\\\\\" + os.getlogin() +\n",
    "    \"\\\\OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey\\\\PainClassifier\"\n",
    ")\n",
    "from my_data_generator import *  # O importa solo lo que uses (recomendado)\n",
    "\n",
    "from vit_keras import vit, utils\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "# Aqui definimos algunos parametros, por ejemplo los de project name y run group son para WANDB\n",
    "PROJECT_NAME = \"Male_W7_vs_W1_ViT\"\n",
    "RUN_GROUP    = \"ViT-B16-50e-pain-only-M-W7_vs_W1\"\n",
    "# Aqui definimos el tamaño de la imagen para pasar al vision transformer\n",
    "# Tambien definimos el batch size y los numeros de epochs \n",
    "IMG_SIZE = 224\n",
    "BATCH    = 32\n",
    "EPOCHS   = 50\n",
    "\n",
    "# Estos son los sujetos que seran usados para el modelo\n",
    "male_all = [57, 60, 73, 74, 93, 94, 95, 96, 98, 99, 100]\n",
    "\n",
    "# Estos son mas parametros en los que indicamos que necesiamtos functional MRI y que sea el de dist y no rest\n",
    "MRI_type = \"func\"\n",
    "functional_type = \"dist\"\n",
    "# En este caso especificamos que necesitamos que sean para week 1 y week 7 ya que 1 es para baselina y 2 es week 1 y 3 es week 7\n",
    "needed_sessions = [2, 3]   \n",
    "\n",
    "# Estas con las especificaciones que se describen en la tesis para poder agarrar archivos\n",
    "TR = 1.51069; ON_SEC, OFF_SEC, CYCLES = 45, 15, 15\n",
    "\n",
    "\n",
    "# En esta funcion basicamente nos ayuda a sacar el substring del archivo como sub-057\n",
    "def subject_id_from_path(p: str):\n",
    "    m = re.search(r\"sub-\\d{3}\", p.replace(\"\\\\\",\"/\"))\n",
    "    if not m: raise ValueError(f\"Could not find subject id in path: {p}\")\n",
    "    return m.group(0)\n",
    "\n",
    "\n",
    "# En esta funcion nos saca por ejemplo: ses-02 y nos regresa el numero de al final que es el 2 y asi sabemos que es de week 1\n",
    "def get_session_from_path(p: str) -> int:\n",
    "    m = re.search(r\"ses-(\\d{2})\", p)\n",
    "    if not m: raise ValueError(f\"Could not find ses-XX in path: {p}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "\n",
    "# Este codigo nos dice o da los indices del fmri que pertenecen al dolor, osea nos da los 135 pertenecientes. \n",
    "# Nos regresa un arrray con los indices\n",
    "\n",
    "\n",
    "def compute_on_indices(T, TR=TR, on_s=ON_SEC, off_s=OFF_SEC, cycles=CYCLES):\n",
    "    task=[]; [task.extend([on_s, off_s]) for _ in range(cycles)]\n",
    "    schedule=[]; t=0\n",
    "    for d in task: t+=d; schedule.append(t)\n",
    "    starts, ends = [], []\n",
    "    for i, s in enumerate(schedule):\n",
    "        if i % 2 == 0: starts.append(int(np.ceil(s/TR) + 1))\n",
    "        else:          ends.append(int(s/TR))\n",
    "    on=[]\n",
    "    for a,b in zip(starts, ends):\n",
    "        a = max(0, min(T-1, a)); b = max(0, min(T-1, b))\n",
    "        if b >= a: on.extend(range(a, b+1))\n",
    "    return np.array(on, dtype=int)\n",
    "\n",
    "# Esta funcion hace para que nos regrese un array de (T,42,65,29)\n",
    "# Carga el 4d fMRI y brain mask \n",
    "\n",
    "def load_masked_cropped(bold_path, mask_path):\n",
    "    img  = nib.load(bold_path)         \n",
    "    mask = nib.load(mask_path)         \n",
    "    bold = np.asarray(img.dataobj,  dtype=np.float32)\n",
    "    msk  = np.asarray(mask.dataobj, dtype=np.float32)\n",
    "    bold = bold * msk[..., None]\n",
    "\n",
    "    bold = bold[3:45, 4:69, 7:36, :]\n",
    "    return np.transpose(bold, (3,0,1,2))  \n",
    "\n",
    "\n",
    "\n",
    "# Esta funcion crea las rebanadas de los volumenes por rata \n",
    "# Se asegura de tener los training data y tambien hace el array de los labels\n",
    "\n",
    "def build_slices_for_pairs(pairs, sub_ses_to_files, window=30, return_ids=False):\n",
    "   \n",
    "    X2D_list, Y_list, IDs_list = [], [], []\n",
    "    for sub, ses, y in pairs:\n",
    "        key = (sub, ses)\n",
    "        if key not in sub_ses_to_files: \n",
    "            continue\n",
    "        bold_path, mask_path = sub_ses_to_files[key]\n",
    "        Txyz = load_masked_cropped(bold_path, mask_path)   \n",
    "        on_idx = compute_on_indices(Txyz.shape[0])        \n",
    "        pain   = Txyz[on_idx]                            \n",
    "\n",
    "        for s in range(0, pain.shape[0], window):\n",
    "            block = pain[s:s+window]                     \n",
    "            if block.shape[0] == 0:\n",
    "                continue\n",
    "            slices_all = np.moveaxis(block, -1, 1).reshape(-1, 42, 65).astype(np.float32)\n",
    "            labels_vec = np.full(slices_all.shape[0], y, dtype=np.int32)\n",
    "            X2D_list.append(slices_all); Y_list.append(labels_vec)\n",
    "            if return_ids:\n",
    "                IDs_list.append(np.array([f\"{sub}_ses-{ses:02d}\"]*len(labels_vec), dtype=object))\n",
    "\n",
    "    X2D = np.concatenate(X2D_list, axis=0) if X2D_list else np.empty((0,42,65), np.float32)\n",
    "    Y   = np.concatenate(Y_list,  axis=0) if Y_list else np.empty((0,), np.int32)\n",
    "    if return_ids:\n",
    "        IDs = np.concatenate(IDs_list, axis=0) if IDs_list else np.empty((0,), dtype=object)\n",
    "        return X2D, Y, IDs\n",
    "    return X2D, Y\n",
    "\n",
    "# Esta funcion hace las ultimas transformaciones para ya pasa al vision transformers\n",
    "def prep(x,y):\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    x = tf.image.resize(x, (IMG_SIZE, IMG_SIZE))\n",
    "    x = tf.image.grayscale_to_rgb(x)\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = (x - tf.reduce_mean(x)) / (tf.math.reduce_std(x) + 1e-6)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# En este caso cargamos el modelo del vision transformer \n",
    "def make_model(vit_variant=\"b16\"):\n",
    "    if vit_variant == \"b16\":\n",
    "        backbone = vit.vit_b16(image_size=IMG_SIZE, pretrained=True, include_top=False, pretrained_top=False)\n",
    "    else:\n",
    "        backbone = vit.vit_b32(image_size=IMG_SIZE, pretrained=True, include_top=False, pretrained_top=False)\n",
    "    inp = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    feat = backbone(inp)\n",
    "    out  = tf.keras.layers.Dense(2, activation=\"softmax\")(feat)\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(3e-5),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "from my_data_generator import FILES_and_LABELS\n",
    "\n",
    "files = FILES_and_LABELS(male_all, needed_sessions, MRI_type, functional_type).get_mask_and_bold()\n",
    "print(f\"Loaded {len(files)} (subject,session) file pairs for DIST ses-02/03.\")\n",
    "\n",
    "# Aqui mapeamos de los valores que tenemos a los archivos\n",
    "sub_ses_to_files = {}\n",
    "for (bold_path, mask_path) in files:\n",
    "    sub = subject_id_from_path(bold_path)\n",
    "    ses = get_session_from_path(bold_path)\n",
    "    sub_ses_to_files[(sub, ses)] = (bold_path, mask_path)\n",
    "\n",
    "# En este caso solo nos quedamos cona los subjects que tenemos W1 and W7\n",
    "subjects_all = sorted({sub for (sub, ses) in sub_ses_to_files.keys()})\n",
    "usable_subjects = np.array([s for s in subjects_all if (s,2) in sub_ses_to_files and (s,3) in sub_ses_to_files])\n",
    "assert len(usable_subjects) > 0, \"No male subjects with both W1 and W7 found.\"\n",
    "\n",
    "print(f\"Usable male subjects with W1 & W7: {list(usable_subjects)} (n={len(usable_subjects)})\")\n",
    "\n",
    "\n",
    "pairs = []\n",
    "for sub in usable_subjects:\n",
    "    pairs.append((sub, 2, 0))   # W1 → 0\n",
    "    pairs.append((sub, 3, 1))   # W7 → 1\n",
    "\n",
    "def subset_pairs_by_subjects(pairs, subj_set):\n",
    "    return [p for p in pairs if p[0] in subj_set]\n",
    "\n",
    "\n",
    "#Aqui usamos el 4 para usar cross validation de 4\n",
    "K = 4\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_idx = 0\n",
    "\n",
    "# En este caso empezamos a entrenar el modelo\n",
    "for train_idx_all, test_idx in kf.split(usable_subjects):\n",
    "    fold_idx += 1\n",
    "\n",
    "    TEST_SUBS   = usable_subjects[test_idx]\n",
    "    TRAIN_SUBS  = usable_subjects[train_idx_all]\n",
    "\n",
    "    \n",
    "    inner = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=SEED+fold_idx)\n",
    "    strat_y = np.array([i % 2 for i in range(len(TRAIN_SUBS))])\n",
    "    tr_inner_idx, val_inner_idx = next(inner.split(TRAIN_SUBS, strat_y))\n",
    "    TR_SUBS = TRAIN_SUBS[tr_inner_idx]\n",
    "    VAL_SUBS= TRAIN_SUBS[val_inner_idx]\n",
    "\n",
    "\n",
    "    TR_PAIRS  = subset_pairs_by_subjects(pairs, set(TR_SUBS))\n",
    "    VAL_PAIRS = subset_pairs_by_subjects(pairs, set(VAL_SUBS))\n",
    "    TE_PAIRS  = subset_pairs_by_subjects(pairs, set(TEST_SUBS))\n",
    "\n",
    "    print(f\"\\n=== Fold {fold_idx}/{K} ===\")\n",
    "    print(\"TEST subs :\", list(TEST_SUBS))\n",
    "    print(\"TRAIN subs:\", list(TR_SUBS))\n",
    "    print(\"VAL subs  :\", list(VAL_SUBS))\n",
    "\n",
    "\n",
    "    X_train, y_train = build_slices_for_pairs(TR_PAIRS, sub_ses_to_files, return_ids=False)\n",
    "    X_val,   y_val   = build_slices_for_pairs(VAL_PAIRS, sub_ses_to_files, return_ids=False)\n",
    "    X_test,  y_test, ids_test = build_slices_for_pairs(TE_PAIRS, sub_ses_to_files, return_ids=True)\n",
    "\n",
    "    print(f\"Fold {fold_idx}: train slices={len(X_train)}, val slices={len(X_val)}, test slices={len(X_test)}\")\n",
    "\n",
    "\n",
    "    ds_train = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                .map(prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .shuffle(min(8192, len(X_train))).batch(BATCH).prefetch(tf.data.AUTOTUNE))\n",
    "    ds_val   = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                .map(prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(BATCH).prefetch(tf.data.AUTOTUNE))\n",
    "    ds_test  = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                .map(prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(BATCH).prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    # Aqui pasamos los datos del modelo al wandB\n",
    "    run = wandb.init(project=PROJECT_NAME,\n",
    "                     group=RUN_GROUP,\n",
    "                     name=f\"M-W7_vs_W1-ViT-B16-fold{fold_idx}\",\n",
    "                     config={\n",
    "                       \"epochs\": EPOCHS,\n",
    "                       \"batch_size\": BATCH,\n",
    "                       \"img_size\": IMG_SIZE,\n",
    "                       \"backbone\": \"vit_b16\",\n",
    "                       \"split\": \"Male W7 (1) vs Male W1 (0), pain-only (DIST)\",\n",
    "                       \"subjects_train\": list(TR_SUBS),\n",
    "                       \"subjects_val\":   list(VAL_SUBS),\n",
    "                       \"subjects_test\":  list(TEST_SUBS),\n",
    "                       \"seed\": SEED\n",
    "                     })\n",
    "\n",
    "    # Aqui llamamos a la funcion de hasta arriba de make model \n",
    "    model = make_model(vit_variant=\"b16\")\n",
    "\n",
    "    # Esto es usando keras-tensorflow que basicamente el modelo corre/entrena\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        callbacks=[WandbCallback(save_model=False)]\n",
    "    )\n",
    "\n",
    "  \n",
    "    # Aqui ya evaluamos \n",
    "    v_loss, v_acc = model.evaluate(ds_val, verbose=0)\n",
    "    print(f\"[Fold {fold_idx}] VAL  → acc={v_acc:.4f}  loss={v_loss:.6f}\")\n",
    "    wandb.log({\"val/final_acc\": v_acc, \"val/final_loss\": v_loss})\n",
    "\n",
    " \n",
    "    y_prob = model.predict(ds_test, verbose=0)        \n",
    "    y_pred = np.argmax(y_prob, axis=1)               \n",
    "    y_score = y_prob[:, 1]                           \n",
    "\n",
    "    acc_slice  = accuracy_score(y_test, y_pred)\n",
    "    bacc_slice = balanced_accuracy_score(y_test, y_pred)\n",
    "    prec_m, rec_m, f1_m, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    try:\n",
    "        auc_slice = roc_auc_score(y_test, y_score)\n",
    "    except Exception:\n",
    "        auc_slice = float(\"nan\")\n",
    "\n",
    "    print(f\"[Fold {fold_idx}] TEST (slice-level) → acc={acc_slice:.4f}  bal_acc={bacc_slice:.4f}  \"\n",
    "          f\"F1_macro={f1_m:.4f}  AUC={auc_slice:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"test_slice/acc\": acc_slice,\n",
    "        \"test_slice/bal_acc\": bacc_slice,\n",
    "        \"test_slice/f1_macro\": f1_m,\n",
    "        \"test_slice/precision_macro\": prec_m,\n",
    "        \"test_slice/recall_macro\": rec_m,\n",
    "        \"test_slice/roc_auc\": auc_slice,\n",
    "    })\n",
    "\n",
    "    # Esto de aqui es para hacer el confusion matrix \n",
    "    cm_slice = confusion_matrix(y_test, y_pred, labels=[0,1])  \n",
    "    print(f\"[Fold {fold_idx}] Confusion matrix (TEST, slice-level):\\n{cm_slice}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm_slice, interpolation='nearest'); plt.title('Confusion matrix (TEST, slice-level)')\n",
    "    plt.colorbar(); tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['W1 (0)','W7 (1)'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['W1 (0)','W7 (1)'])\n",
    "    for i in range(cm_slice.shape[0]):\n",
    "        for j in range(cm_slice.shape[1]):\n",
    "            val = cm_slice[i, j]\n",
    "            plt.text(j, i, val, ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if val > cm_slice.max()/2 else \"black\")\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label'); plt.tight_layout()\n",
    "    wandb.log({\"test_slice/confusion_matrix\": wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d76b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# W7 Male (1) vs W1 Female (0) — pain-only (DIST)\n",
    "# 4-fold CV, 50 epochs, ViT-B/16, Stratified by class\n",
    "# ================================================\n",
    "\n",
    "\n",
    "# Estos son los imports que se necesitan para poder activar el codido o poder usarlo\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "sys.modules['keras'] = tf.keras  # Para que vit_keras use tf.keras internamente\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Ruta a tu proyecto (PainClassifier)\n",
    "sys.path.append(\n",
    "    \"C:\\\\Users\\\\\" + os.getlogin() +\n",
    "    \"\\\\OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey\\\\PainClassifier\"\n",
    ")\n",
    "from my_data_generator import *  # O importa solo lo que uses (recomendado)\n",
    "\n",
    "from vit_keras import vit, utils\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "# Aqui definimos algunos parametros, por ejemplo los de project name y run group son para WANDB\n",
    "PROJECT_NAME = \"W7Male_vs_W1Female_ViT\"\n",
    "RUN_GROUP    = \"ViT-B16-50e-pain-only-W7M_vs_W1F\"\n",
    "# Aqui definimos el tamaño de la imagen para pasar al vision transformer\n",
    "# Tambien definimos el batch size y los numeros de epochs \n",
    "IMG_SIZE = 224\n",
    "BATCH    = 32\n",
    "EPOCHS   = 50\n",
    "\n",
    "# Estos son los sujetos que seran usados para el modelo\n",
    "male   = [57, 59, 60, 73, 74, 93, 94, 95, 96, 98, 99, 100]  \n",
    "female = [49, 50, 51, 52, 65, 66, 77, 78, 79, 80, 81, 82]   \n",
    "\n",
    "# En este caso decimos que el uno es para los males y los 0 para los females \n",
    "# Es un diccionario que nos mapea dependiondo del subject a donde deberia de ir \n",
    "label_map_subject = {**{f\"sub-{m:03d}\": 1 for m in male},  \n",
    "                     **{f\"sub-{f_:03d}\": 0 for f_ in female}}  \n",
    "\n",
    "\n",
    "MRI_type = \"func\"\n",
    "functional_type = \"dist\"\n",
    "needed_sessions = [2, 3]   \n",
    "\n",
    "\n",
    "TR = 1.51069; ON_SEC, OFF_SEC, CYCLES = 45, 15, 15\n",
    "\n",
    "def subject_id_from_path(p: str):\n",
    "    m = re.search(r\"sub-\\d{3}\", p.replace(\"\\\\\",\"/\"))\n",
    "    if not m: raise ValueError(f\"Could not find subject id in path: {p}\")\n",
    "    return m.group(0)\n",
    "\n",
    "def get_session_from_path(p: str) -> int:\n",
    "    m = re.search(r\"ses-(\\d{2})\", p)\n",
    "    if not m: raise ValueError(f\"Could not find ses-XX in path: {p}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def compute_on_indices(T, TR=TR, on_s=ON_SEC, off_s=OFF_SEC, cycles=CYCLES):\n",
    "    task=[]; [task.extend([on_s, off_s]) for _ in range(cycles)]\n",
    "    schedule=[]; t=0\n",
    "    for d in task: t+=d; schedule.append(t)\n",
    "    starts, ends = [], []\n",
    "    for i, s in enumerate(schedule):\n",
    "        if i % 2 == 0: starts.append(int(np.ceil(s/TR) + 1))\n",
    "        else:          ends.append(int(s/TR))\n",
    "    on=[]\n",
    "    for a,b in zip(starts, ends):\n",
    "        a = max(0, min(T-1, a)); b = max(0, min(T-1, b))\n",
    "        if b >= a: on.extend(range(a, b+1))\n",
    "    return np.array(on, dtype=int)\n",
    "\n",
    "def load_masked_cropped(bold_path, mask_path):\n",
    "    img  = nib.load(bold_path)         \n",
    "    mask = nib.load(mask_path)         \n",
    "    bold = np.asarray(img.dataobj,  dtype=np.float32)\n",
    "    msk  = np.asarray(mask.dataobj, dtype=np.float32)\n",
    "    bold = bold * msk[..., None]\n",
    "\n",
    "    bold = bold[3:45, 4:69, 7:36, :]\n",
    "    return np.transpose(bold, (3,0,1,2)) \n",
    "\n",
    "def build_slices_for_pairs(pairs, sub_ses_to_files, window=30, return_ids=False):\n",
    "    \"\"\"\n",
    "    pairs = [(sub_id, ses_int, class_label), ...]\n",
    "    For each (sub, ses), take pain-only windows (~135 ON frames) split into 30-vol blocks.\n",
    "    \"\"\"\n",
    "    X2D_list, Y_list, IDs_list = [], [], []\n",
    "    for sub, ses, y in pairs:\n",
    "        key = (sub, ses)\n",
    "        if key not in sub_ses_to_files:  \n",
    "            continue\n",
    "        bold_path, mask_path = sub_ses_to_files[key]\n",
    "        Txyz = load_masked_cropped(bold_path, mask_path)  \n",
    "        on_idx = compute_on_indices(Txyz.shape[0])      \n",
    "        pain   = Txyz[on_idx]                              \n",
    "\n",
    "      \n",
    "        for s in range(0, pain.shape[0], window):\n",
    "            block = pain[s:s+window]                     \n",
    "            if block.shape[0] == 0: \n",
    "                continue\n",
    "            slices_all = np.moveaxis(block, -1, 1).reshape(-1, 42, 65).astype(np.float32)\n",
    "            labels_vec = np.full(slices_all.shape[0], y, dtype=np.int32)\n",
    "            X2D_list.append(slices_all); Y_list.append(labels_vec)\n",
    "            if return_ids:\n",
    "                IDs_list.append(np.array([f\"{sub}_ses-{ses:02d}\"]*len(labels_vec), dtype=object))\n",
    "\n",
    "    X2D = np.concatenate(X2D_list, axis=0) if X2D_list else np.empty((0,42,65), np.float32)\n",
    "    Y   = np.concatenate(Y_list,  axis=0) if Y_list else np.empty((0,), np.int32)\n",
    "    if return_ids:\n",
    "        IDs = np.concatenate(IDs_list, axis=0) if IDs_list else np.empty((0,), dtype=object)\n",
    "        return X2D, Y, IDs\n",
    "    return X2D, Y\n",
    "\n",
    "def prep(x,y):\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    x = tf.image.resize(x, (IMG_SIZE, IMG_SIZE))\n",
    "    x = tf.image.grayscale_to_rgb(x)\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = (x - tf.reduce_mean(x)) / (tf.math.reduce_std(x) + 1e-6)\n",
    "    return x, y\n",
    "\n",
    "def make_model(vit_variant=\"b16\"):\n",
    "    if vit_variant == \"b16\":\n",
    "        backbone = vit.vit_b16(image_size=IMG_SIZE, pretrained=True, include_top=False, pretrained_top=False)\n",
    "    else:\n",
    "        backbone = vit.vit_b32(image_size=IMG_SIZE, pretrained=True, include_top=False, pretrained_top=False)\n",
    "    inp = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    feat = backbone(inp)\n",
    "    out  = tf.keras.layers.Dense(2, activation=\"softmax\")(feat)\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(3e-5),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "from my_data_generator import FILES_and_LABELS\n",
    "\n",
    "# En este caso ahora cargamos los machos y hembras \n",
    "files = FILES_and_LABELS(male + female, needed_sessions, MRI_type, functional_type).get_mask_and_bold()\n",
    "print(f\"Loaded {len(files)} (subject,session) file pairs for DIST ses-02/03.\")\n",
    "\n",
    "\n",
    "sub_ses_to_files = {}\n",
    "for (bold_path, mask_path) in files:\n",
    "    sub = subject_id_from_path(bold_path)\n",
    "    ses = get_session_from_path(bold_path)\n",
    "    sub_ses_to_files[(sub, ses)] = (bold_path, mask_path)\n",
    "\n",
    "\n",
    "# Aqui construimos la lista de de pares en el cual si es de machos semana 7 o numero 3 entonces es 1\n",
    "# Si es hembra semana 1 pues es 0\n",
    "\n",
    "male_pairs   = [(f\"sub-{m:03d}\", 3, 1) for m in male   if (f\"sub-{m:03d}\", 3) in sub_ses_to_files]\n",
    "female_pairs = [(f\"sub-{f_:03d}\", 2, 0) for f_ in female if (f\"sub-{f_:03d}\", 2) in sub_ses_to_files]\n",
    "\n",
    "pairs = male_pairs + female_pairs\n",
    "assert len(male_pairs) > 0 and len(female_pairs) > 0, \"Missing W7 males or W1 females.\"\n",
    "\n",
    "\n",
    "\n",
    "# Aqui extraemos los sujetos unicos con sus arreglos de IDs y su etiqueta \n",
    "# Esto para ayudar con el stratified k fold\n",
    "\n",
    "units   = np.array([p[0] for p in pairs])       \n",
    "y_units = np.array([p[2] for p in pairs])       \n",
    "\n",
    "\n",
    "uniq, idx = np.unique(units, return_index=True)\n",
    "units = units[idx]; y_units = y_units[idx]\n",
    "\n",
    "print(f\"Usable subjects: {list(units)}\")\n",
    "print(f\"Counts → class1(♂W7): {int(y_units.sum())}, class0(♀W1): {len(y_units)-int(y_units.sum())}\")\n",
    "\n",
    "\n",
    "def subset_pairs_by_subjects(pairs, subj_set):\n",
    "    return [p for p in pairs if p[0] in subj_set]\n",
    "\n",
    "\n",
    "K = 4\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_idx = 0\n",
    "for train_idx_all, test_idx in skf.split(units, y_units):\n",
    "    fold_idx += 1\n",
    "\n",
    "    TEST_SUBS   = units[test_idx]\n",
    "    TEST_LABELS = y_units[test_idx]   \n",
    "    TRAIN_SUBS  = units[train_idx_all]\n",
    "\n",
    "    \n",
    "    train_labels = y_units[train_idx_all]\n",
    "    inner = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=SEED+fold_idx)\n",
    "    tr_inner_idx, val_inner_idx = next(inner.split(TRAIN_SUBS, train_labels))\n",
    "    TR_SUBS = TRAIN_SUBS[tr_inner_idx]\n",
    "    VAL_SUBS= TRAIN_SUBS[val_inner_idx]\n",
    "\n",
    "    TR_PAIRS  = subset_pairs_by_subjects(pairs, set(TR_SUBS))\n",
    "    VAL_PAIRS = subset_pairs_by_subjects(pairs, set(VAL_SUBS))\n",
    "    TE_PAIRS  = subset_pairs_by_subjects(pairs, set(TEST_SUBS))\n",
    "\n",
    "    print(f\"\\n=== Fold {fold_idx}/{K} ===\")\n",
    "    print(\"TEST subs :\", list(TEST_SUBS), \" (♂W7:\", int(TEST_LABELS.sum()), \" / ♀W1:\", len(TEST_LABELS)-int(TEST_LABELS.sum()), \")\")\n",
    "    print(\"TRAIN subs:\", list(TR_SUBS))\n",
    "    print(\"VAL subs  :\", list(VAL_SUBS))\n",
    "\n",
    "\n",
    "    X_train, y_train = build_slices_for_pairs(TR_PAIRS, sub_ses_to_files, return_ids=False)\n",
    "    X_val,   y_val   = build_slices_for_pairs(VAL_PAIRS, sub_ses_to_files, return_ids=False)\n",
    "    X_test,  y_test, ids_test = build_slices_for_pairs(TE_PAIRS, sub_ses_to_files, return_ids=True)\n",
    "\n",
    "    print(f\"Fold {fold_idx}: train slices={len(X_train)}, val slices={len(X_val)}, test slices={len(X_test)}\")\n",
    "\n",
    "\n",
    "    def prep(x,y):\n",
    "        x = tf.expand_dims(x, -1)\n",
    "        x = tf.image.resize(x, (IMG_SIZE, IMG_SIZE))\n",
    "        x = tf.image.grayscale_to_rgb(x)\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        x = (x - tf.reduce_mean(x)) / (tf.math.reduce_std(x) + 1e-6)\n",
    "        return x, y\n",
    "\n",
    "    ds_train = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                .map(prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .shuffle(min(8192, len(X_train))).batch(BATCH).prefetch(tf.data.AUTOTUNE))\n",
    "    ds_val   = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                .map(prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(BATCH).prefetch(tf.data.AUTOTUNE))\n",
    "    ds_test  = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                .map(prep, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(BATCH).prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "   \n",
    "    run = wandb.init(project=PROJECT_NAME,\n",
    "                     group=RUN_GROUP,\n",
    "                     name=f\"W7M_vs_W1F-ViT-B16-fold{fold_idx}\",\n",
    "                     config={\n",
    "                       \"epochs\": EPOCHS,\n",
    "                       \"batch_size\": BATCH,\n",
    "                       \"img_size\": IMG_SIZE,\n",
    "                       \"backbone\": \"vit_b16\",\n",
    "                       \"split\": \"W7 Male (1) vs W1 Female (0), pain-only (DIST)\",\n",
    "                       \"subjects_train\": list(TR_SUBS),\n",
    "                       \"subjects_val\":   list(VAL_SUBS),\n",
    "                       \"subjects_test\":  list(TEST_SUBS),\n",
    "                       \"seed\": SEED\n",
    "                     })\n",
    "\n",
    "\n",
    "    model = make_model(vit_variant=\"b16\")\n",
    "\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        callbacks=[WandbCallback(save_model=False)]\n",
    "    )\n",
    "\n",
    "   \n",
    "    v_loss, v_acc = model.evaluate(ds_val, verbose=0)\n",
    "    print(f\"[Fold {fold_idx}] VAL  → acc={v_acc:.4f}  loss={v_loss:.6f}\")\n",
    "    wandb.log({\"val/final_acc\": v_acc, \"val/final_loss\": v_loss})\n",
    "\n",
    "    y_prob = model.predict(ds_test, verbose=0)     \n",
    "    y_pred = np.argmax(y_prob, axis=1)               \n",
    "    y_score = y_prob[:, 1]                           \n",
    "\n",
    "    acc_slice  = accuracy_score(y_test, y_pred)\n",
    "    bacc_slice = balanced_accuracy_score(y_test, y_pred)\n",
    "    prec_m, rec_m, f1_m, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    try:\n",
    "        auc_slice = roc_auc_score(y_test, y_score)\n",
    "    except Exception:\n",
    "        auc_slice = float(\"nan\")\n",
    "\n",
    "    print(f\"[Fold {fold_idx}] TEST (slice-level) → acc={acc_slice:.4f}  bal_acc={bacc_slice:.4f}  \"\n",
    "          f\"F1_macro={f1_m:.4f}  AUC={auc_slice:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"test_slice/acc\": acc_slice,\n",
    "        \"test_slice/bal_acc\": bacc_slice,\n",
    "        \"test_slice/f1_macro\": f1_m,\n",
    "        \"test_slice/precision_macro\": prec_m,\n",
    "        \"test_slice/recall_macro\": rec_m,\n",
    "        \"test_slice/roc_auc\": auc_slice,\n",
    "    })\n",
    "\n",
    "    \n",
    "    cm_slice = confusion_matrix(y_test, y_pred, labels=[0,1])  \n",
    "    print(f\"[Fold {fold_idx}] Confusion matrix (TEST, slice-level):\\n{cm_slice}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm_slice, interpolation='nearest'); plt.title('Confusion matrix (TEST, slice-level)')\n",
    "    plt.colorbar(); tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Female W1 (0)','Male W7 (1)'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['Female W1 (0)','Male W7 (1)'])\n",
    "    for i in range(cm_slice.shape[0]):\n",
    "        for j in range(cm_slice.shape[1]):\n",
    "            val = cm_slice[i, j]\n",
    "            plt.text(j, i, val, ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if val > cm_slice.max()/2 else \"black\")\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label'); plt.tight_layout()\n",
    "    wandb.log({\"test_slice/confusion_matrix\": wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "\n",
    "    run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
